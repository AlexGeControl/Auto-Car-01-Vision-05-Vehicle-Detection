{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Self-Driving Car Engineer Nanodegree\n",
    "***\n",
    "\n",
    "## Project: **Vehicle Detection** \n",
    "***\n",
    "\n",
    "In this project, classic object detection framework, i.e., sliding window + image pyramid region proposer and HOG feature + calibrated linear SVM classifier will be implemented and used for vehicle detection.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Up Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Configuration file:\n",
    "from vehicle_detection.utils.conf import Conf\n",
    "# IO utilities:\n",
    "import random\n",
    "import glob\n",
    "import matplotlib.image as mpimg\n",
    "from vehicle_detection.utils.dataset import to_hdf5, read_hdf5\n",
    "import pickle\n",
    "# Image processing:\n",
    "import numpy as np\n",
    "import cv2\n",
    "from vehicle_detection.extractors import ReshapeTransformer\n",
    "from vehicle_detection.extractors import ColorHistogramTransformer\n",
    "from vehicle_detection.extractors import HOGTransformer\n",
    "from vehicle_detection.extractors import TemplateTransformer\n",
    "# Visualization:\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "conf = Conf(\"conf/vehicles.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore Dataset\n",
    "***\n",
    "\n",
    "First, explore the dataset for vehicle classifier building."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Vehicle images:\n",
    "vehicle_filenames = glob.glob(conf.vehicle_dataset)\n",
    "# Non-vehicle images:\n",
    "non_vehicle_filenames = glob.glob(conf.non_vehicle_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Vehicle sample:\n",
    "vehicle_image_sample = mpimg.imread(\n",
    "    random.choice(\n",
    "        vehicle_filenames\n",
    "    )\n",
    ")\n",
    "# Non-vehicle sample:\n",
    "non_vehicle_image_sample = mpimg.imread(\n",
    "    random.choice(\n",
    "        non_vehicle_filenames\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset samples:\n",
    "dataset_samples_demo = plt.figure(figsize=(4, 3))\n",
    "# Vehicle sample:\n",
    "ax=dataset_samples_demo.add_subplot(1,2,1)\n",
    "plt.imshow(vehicle_image_sample)\n",
    "ax.set_title('Vehicle')\n",
    "# Non-vehicle sample\n",
    "ax=dataset_samples_demo.add_subplot(1,2,2)\n",
    "plt.imshow(non_vehicle_image_sample)\n",
    "ax.set_title('Non-Vehicle')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After viewing samples from the dataset, we know that **all the cars in images have been clearly segmented**. Thus **HOG features can be extracted directly from input image**.\n",
    "\n",
    "We still need to **set the window size for HOG extractor**. Besides, dataset composition should also be evaluated(e.g., whether the dataset is imbalanced) so as to select the proper algorithm for classifier building.\n",
    "\n",
    "The two stats can be attained from the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vehicle image stats:\n",
    "print(\n",
    "    \"[  Vehicle Images  ]: Num--{}, Dimensions--{}\".format(\n",
    "        len(vehicle_filenames),\n",
    "        np.array(\n",
    "            [mpimg.imread(vehicle_filename).shape for vehicle_filename in vehicle_filenames]\n",
    "        ).mean(axis = 0)\n",
    "    )\n",
    ")\n",
    "# Non-vehicle image stats:\n",
    "print(\n",
    "    \"[Non-Vehicle Images]: Num--{}, Dimensions--{}\".format(\n",
    "        len(non_vehicle_filenames),\n",
    "        np.array(\n",
    "            [mpimg.imread(non_vehicle_filename).shape for non_vehicle_filename in non_vehicle_filenames]\n",
    "        ).mean(axis = 0)\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above output we know that:\n",
    "\n",
    "**1. Window size for HOG extractor should be set as 64-by-64;**\n",
    "\n",
    "**2. There are 8792 positive images and 8968 negative images in training dataset. The dataset is approximately balanced.**\n",
    "\n",
    "***\n",
    "\n",
    "Next let's try to identify the best color space for vehicle & non-vehicle color feature extraction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Set up session:\n",
    "from vehicle_detection.detectors.image_processing import resize\n",
    "from vehicle_detection.utils.visualization import plot_3d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Utilities for color space exploration:\n",
    "def parse_conversion(color_space):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    if color_space == \"HSV\":\n",
    "        return (cv2.COLOR_BGR2HSV, (\"H\", \"S\", \"V\"))\n",
    "    elif color_space == \"Lab\":\n",
    "        return (cv2.COLOR_BGR2Lab, (\"L*\", \"a*\", \"b*\"))\n",
    "    else:\n",
    "        return (cv2.COLOR_BGR2RGB, (\"R\", \"G\", \"B\"))\n",
    "\n",
    "def plot_pixel_distribution(image_filename, color_space):\n",
    "    # Read:\n",
    "    image_BGR = cv2.imread(image_filename)    \n",
    "    \n",
    "    # Parse conversion:\n",
    "    (conversion, channels) = parse_conversion(color_space)\n",
    "\n",
    "    # Convert subsampled image to desired color space(s):\n",
    "    img_RGB = cv2.cvtColor(image_BGR, cv2.COLOR_BGR2RGB)  # OpenCV uses BGR, matplotlib likes RGB\n",
    "    img_color_space = cv2.cvtColor(image_BGR, conversion)\n",
    "    colors = img_RGB / 255.  # scaled to [0, 1], only for plotting\n",
    "\n",
    "    # Plot and show:\n",
    "    plot_3d(img_color_space, colors, axis_labels=channels)\n",
    "    plt.show()\n",
    "\n",
    "def explore_pixel_distribution(vehicle_filenames, non_vehicle_filenames, color_space):\n",
    "    import random\n",
    "    # Vehicles:\n",
    "    plot_pixel_distribution(random.choice(vehicle_filenames), color_space)\n",
    "    # Non-vehicles:\n",
    "    plot_pixel_distribution(random.choice(non_vehicle_filenames), color_space)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here YUV is selected since it is a homogeneous space and L2 norm has a clear meaning as color distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explore_pixel_distribution(vehicle_filenames, non_vehicle_filenames, \"YUV\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "Below are the HOG descriptions on all three channel components on vehicle & non-vehicle image samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extract_channel_component_and_hog_image(image):\n",
    "    \"\"\" Extract channel components and corresponding hog images for visualization\n",
    "    \"\"\"\n",
    "    from skimage.feature import hog\n",
    "    \n",
    "    image_yuv = cv2.cvtColor(image, cv2.COLOR_RGB2YUV)\n",
    "    \n",
    "    channel_components = []\n",
    "    hog_images = []\n",
    "    \n",
    "    for channel_component in cv2.split(image_yuv):\n",
    "        channel_components.append(channel_component)\n",
    "        _, hog_image = hog(\n",
    "            channel_component,\n",
    "            orientations = conf.hog_orientations,\n",
    "            pixels_per_cell = conf.hog_pixels_per_cell,\n",
    "            cells_per_block = conf.hog_cells_per_block,\n",
    "            transform_sqrt = conf.hog_normalize,\n",
    "            block_norm = conf.hog_block_norm,\n",
    "            visualise = True\n",
    "        )\n",
    "        hog_images.append(hog_image)\n",
    "    \n",
    "    return (channel_components, hog_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract descriptions:\n",
    "(\n",
    "    vehicle_channel_components, \n",
    "    vehicle_hog_images\n",
    ") = extract_channel_component_and_hog_image(vehicle_image_sample)\n",
    "(\n",
    "    non_vehicle_channel_components, \n",
    "    non_vehicle_hog_images\n",
    ") = extract_channel_component_and_hog_image(non_vehicle_image_sample)\n",
    "\n",
    "# Canvas for HOG demo:\n",
    "hog_demo = plt.figure(figsize=(16, 9))\n",
    "\n",
    "# Visualize:\n",
    "for channel_id, (\n",
    "    vehicle_channel_component,\n",
    "    vehicle_hog_image,\n",
    "    non_vehicle_channel_component,\n",
    "    non_vehicle_hog_image\n",
    ") in enumerate(\n",
    "    zip(\n",
    "        vehicle_channel_components, \n",
    "        vehicle_hog_images, \n",
    "        non_vehicle_channel_components, \n",
    "        non_vehicle_hog_images\n",
    "    )\n",
    "):\n",
    "    # Channel name:\n",
    "    channel_name = \"YUV\"[channel_id]\n",
    "    \n",
    "    # Vehicle sample: \n",
    "    ax=hog_demo.add_subplot(3,4,4*channel_id + 1)\n",
    "    plt.imshow(vehicle_channel_component)\n",
    "    ax.set_title(\"Vehicle--{}-Component\".format(channel_name))\n",
    "    # Non-vehicle sample\n",
    "    ax=hog_demo.add_subplot(3,4,4*channel_id + 2)\n",
    "    plt.imshow(vehicle_hog_image)\n",
    "    ax.set_title(\"Vehicle--{}-HOG-Image\".format(channel_name))\n",
    "    # Vehicle sample: \n",
    "    ax=hog_demo.add_subplot(3,4,4*channel_id + 3)\n",
    "    plt.imshow(non_vehicle_channel_component)\n",
    "    ax.set_title(\"Non-Vehicle--{}-Component\".format(channel_name))\n",
    "    # Non-vehicle sample\n",
    "    ax=hog_demo.add_subplot(3,4,4*channel_id + 4)\n",
    "    plt.imshow(non_vehicle_hog_image)\n",
    "    ax.set_title(\"Non-Vehicle--{}-HOG-Image\".format(channel_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Training Dataset\n",
    "\n",
    "***\n",
    "\n",
    "Build the dataset for vehicle classifier training.\n",
    "\n",
    "The loaded images are first serialized as 1d vector to bypass CalibratedClassifierCV's integrity check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Utilities:\n",
    "def downsample(\n",
    "    image_filenames, \n",
    "    sampling_percentange\n",
    "):\n",
    "    \"\"\" Sample image files\n",
    "    \"\"\"\n",
    "    # Down-sample:\n",
    "    image_filenames = np.random.choice(\n",
    "        image_filenames, \n",
    "        int(sampling_percentange * len(image_filenames))\n",
    "    )\n",
    "    \n",
    "    return image_filenames\n",
    "\n",
    "def load_images(\n",
    "    image_filenames,\n",
    "    image_size,\n",
    "    augmentation=True\n",
    "):\n",
    "    \"\"\" Load images\n",
    "    \"\"\"\n",
    "    features = []\n",
    "    \n",
    "    # Extract features:\n",
    "    for image_filename in image_filenames:\n",
    "        # Load and convert to grayscale:\n",
    "        object_image = cv2.resize(\n",
    "            cv2.imread(image_filename),\n",
    "            image_size,\n",
    "            interpolation = cv2.INTER_AREA\n",
    "        )\n",
    "        # Prepare ROIs:\n",
    "        ROIs = (object_image, cv2.flip(object_image, 1)) if augmentation else (object_image,)\n",
    "        # Extract features:\n",
    "        for ROI in ROIs:\n",
    "            features.append(ROI)\n",
    "    \n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Should dataset be extracted:\n",
    "if conf.generate_dataset:\n",
    "    # Load images:\n",
    "    vehicle_images = load_images(\n",
    "        downsample(vehicle_filenames, sampling_percentange=conf.sampling_percentange),\n",
    "        tuple(conf.hog_window_size),\n",
    "        conf.augmentation\n",
    "    )\n",
    "    non_vehicle_images = load_images(\n",
    "        downsample(non_vehicle_filenames, sampling_percentange=conf.sampling_percentange),\n",
    "        tuple(conf.hog_window_size),\n",
    "        conf.augmentation\n",
    "    )\n",
    "    # Training set:\n",
    "    X_train = np.array(vehicle_images + non_vehicle_images)\n",
    "    y_train = np.array([1] * len(vehicle_images) + [-1] * len(non_vehicle_images))\n",
    "    indices = np.arange(len(X_train))\n",
    "    np.random.shuffle(indices)\n",
    "    X_train, y_train = X_train[indices], y_train[indices]\n",
    "    # Shape:\n",
    "    X_train = X_train.reshape(tuple(conf.shape_serialized))\n",
    "    # Dataset info:\n",
    "    print(X_train.shape)\n",
    "    print(y_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Classifier\n",
    "\n",
    "***\n",
    "\n",
    "Here I choose to implement linear SVM using LinearSVC because the dimensions of training dataset,(35520, -1), is formidable. Use SVC will lead to a very slow training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross validation:\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "# Classifier:\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "# Evaluation metric:\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import make_scorer\n",
    "# Hyperparameter tuning:\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Model 1--Linear SVC:\n",
    "def get_linear_svc():\n",
    "    # Model:\n",
    "    model = Pipeline(\n",
    "        [\n",
    "            # Deserializer:\n",
    "            ('des', ReshapeTransformer(conf.shape_deserialized)),\n",
    "            # Feature extractor:\n",
    "            ('vec', FeatureUnion(\n",
    "                [\n",
    "                    (\"hog\", HOGTransformer(\n",
    "                        color_space = conf.hog_color_space,\n",
    "                        shape_only = conf.hog_shape_only,\n",
    "                        orientations = conf.hog_orientations,\n",
    "                        pixels_per_cell = tuple(conf.hog_pixels_per_cell),\n",
    "                        cells_per_block = tuple(conf.hog_cells_per_block),\n",
    "                        transform_sqrt = conf.hog_normalize,\n",
    "                        block_norm = str(conf.hog_block_norm)\n",
    "                    )),\n",
    "                ]\n",
    "            )),\n",
    "            # Preprocessor:\n",
    "            ('scl', StandardScaler()),\n",
    "            # Classifier:\n",
    "            ('clf', LinearSVC(\n",
    "                penalty='l2', \n",
    "                loss=conf.classifier_loss,\n",
    "                C=conf.classifier_C,\n",
    "                max_iter=2000\n",
    "            ))\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # Hyperparameters:\n",
    "    params = {\n",
    "        # VEC--hog:\n",
    "        #\"vec__hog__pixels_per_cell\": ((8,8), (16, 16)),\n",
    "        # CLF--learning rate:\n",
    "        #\"clf__loss\": (\"hinge\", \"squared_hinge\"),\n",
    "        # CLF--regularization:\n",
    "        #\"clf__penalty\": (\"l1\", \"l2\")\n",
    "        \"clf__C\": (5e-4, 1e-3)\n",
    "    }\n",
    "    \n",
    "    return (model, params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Model 2--XGBoost:\n",
    "def get_xgboost():\n",
    "    # Model:\n",
    "    model = Pipeline(\n",
    "        [\n",
    "            # Deserializer:\n",
    "            ('des', ReshapeTransformer(conf.shape_deserialized)),\n",
    "            # Feature extractor:\n",
    "            ('vec', FeatureUnion(\n",
    "                [\n",
    "                    # 2. Shape--HOG:\n",
    "                    (\"hog\", HOGTransformer(\n",
    "                        color_space = conf.hog_color_space,\n",
    "                        shape_only = conf.hog_shape_only,\n",
    "                        orientations = conf.hog_orientations,\n",
    "                        pixels_per_cell = tuple(conf.hog_pixels_per_cell),\n",
    "                        cells_per_block = tuple(conf.hog_cells_per_block),\n",
    "                        transform_sqrt = conf.hog_normalize,\n",
    "                        block_norm = str(conf.hog_block_norm)\n",
    "                    )),\n",
    "                ]\n",
    "            )),\n",
    "            # Preprocessor:\n",
    "            ('scl', StandardScaler()),\n",
    "            # Classifier:\n",
    "            ('clf', XGBClassifier(\n",
    "                max_depth=8, \n",
    "                learning_rate=0.1, \n",
    "                n_estimators=1024,\n",
    "                nthread=4\n",
    "            ))\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # Hyperparameters:\n",
    "    params = {\n",
    "        # VEC--hog:\n",
    "        #\"vec__hog__pixels_per_cell\": ((8,8), (16, 16)),\n",
    "        # CLF--learning rate:\n",
    "        #\"clf__learning_rate\": (0.1, 0.3),\n",
    "    }\n",
    "    \n",
    "    return (model, params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Model 3--Logistic regression:\n",
    "def get_logistic():\n",
    "    # Model:\n",
    "    model = Pipeline(\n",
    "        [\n",
    "            # Deserializer:\n",
    "            ('des', ReshapeTransformer(conf.shape_deserialized)),\n",
    "            # Feature extractor:\n",
    "            ('vec', FeatureUnion(\n",
    "                [\n",
    "                    # 2. Shape--HOG:\n",
    "                    (\"hog\", HOGTransformer(\n",
    "                        color_space = conf.hog_color_space,\n",
    "                        shape_only = conf.hog_shape_only,\n",
    "                        orientations = conf.hog_orientations,\n",
    "                        # Optimal--(8, 8):\n",
    "                        pixels_per_cell = tuple(conf.hog_pixels_per_cell),\n",
    "                        cells_per_block = tuple(conf.hog_cells_per_block),\n",
    "                        # Optimal--True:\n",
    "                        transform_sqrt = conf.hog_normalize,\n",
    "                        block_norm = str(conf.hog_block_norm)\n",
    "                    )),\n",
    "                ]\n",
    "            )),\n",
    "            # Preprocessor:\n",
    "            ('scl', StandardScaler()),\n",
    "            # Classifier:\n",
    "            ('clf', LogisticRegression(\n",
    "                penalty='l2', \n",
    "                C=1.0,\n",
    "                n_jobs=4 \n",
    "            ))\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # Hyperparameters:\n",
    "    params = {\n",
    "        # VEC--hog:\n",
    "        #\"vec__hog__pixels_per_cell\": ((8,8), (16, 16)),\n",
    "        # CLF--learning rate:\n",
    "        #\"clf__loss\": (\"hinge\", \"squared_hinge\"),\n",
    "        # CLF--regularization:\n",
    "        #\"clf__penalty\": (\"l1\", \"l2\")\n",
    "        \"clf__C\": (1e-3, 1e-1)\n",
    "    }\n",
    "    \n",
    "    return (model, params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create cross-validation sets from the training data\n",
    "cv_sets_training = StratifiedShuffleSplit(\n",
    "    n_splits = 3, \n",
    "    test_size = 0.20, \n",
    "    random_state = 42\n",
    ").split(X_train, y_train)\n",
    "\n",
    "# Model 1: Linear SVC\n",
    "(model, params) = get_linear_svc()\n",
    "# Model 2: XGBoost:\n",
    "#(model, params) = get_xgboost()\n",
    "# Model 3: Logistic\n",
    "#(model, params) = get_logistic()\n",
    "\n",
    "# Make an scorer object\n",
    "scorer = make_scorer(accuracy_score)\n",
    "\n",
    "# Perform grid search on the classifier using 'scorer' as the scoring method\n",
    "grid_searcher = GridSearchCV(\n",
    "    estimator = model,\n",
    "    param_grid = params,\n",
    "    scoring = scorer,\n",
    "    cv = cv_sets_training,\n",
    "    n_jobs = 2,\n",
    "    verbose = 10\n",
    ")\n",
    "\n",
    "# Fit the grid search object to the training data and find the optimal parameters\n",
    "grid_fitted = grid_searcher.fit(X_train, y_train)\n",
    "\n",
    "# Get parameters & scores:\n",
    "best_parameters, score, _ = max(grid_fitted.grid_scores_, key=lambda x: x[1])\n",
    "\n",
    "# Display result:\n",
    "print(\n",
    "    \"[Best Parameters]: {}\\n[Best Score]: {}\".format(\n",
    "        best_parameters, score\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"[Train & Calibrate Best Model]: ...\")\n",
    "# Get the best model\n",
    "best_model = grid_fitted.best_estimator_\n",
    "best_model.set_params(**best_parameters)\n",
    "\n",
    "# Train on whole dataset with best parameters and probability calibration:\n",
    "best_model_calibrated = CalibratedClassifierCV(best_model, cv=3)\n",
    "best_model_calibrated.fit(X_train, y_train)\n",
    "print(\"[Train & Calibrate Best Model]: Done.\")\n",
    "\n",
    "# Save model:\n",
    "with open(conf.classifier_path, 'wb') as model_pkl:\n",
    "    pickle.dump(best_model_calibrated, model_pkl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vehicle Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Set up session:\n",
    "from vehicle_detection.detectors import SlidingWindowPyramidDetector\n",
    "from vehicle_detection.detectors import non_maxima_suppression\n",
    "from vehicle_detection.detectors import heatmap_filtering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Detector:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Initialize detector:\n",
    "detector = SlidingWindowPyramidDetector(\n",
    "    conf\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test on Static Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Utilities:\n",
    "def detect_vehicle(image, detector, heat_thresh=None):    \n",
    "    # Detect:\n",
    "    bounding_boxes = detector.detect(\n",
    "        image\n",
    "    )\n",
    "    \n",
    "    # Heatmap filtering:\n",
    "    if not heat_thresh is None:\n",
    "        bounding_boxes = heatmap_filtering(image, bounding_boxes, heat_thresh)\n",
    "        \n",
    "    # Draw:\n",
    "    canvas = image.copy()\n",
    "    for bounding_box in bounding_boxes:\n",
    "        (top, bottom, left, right) = bounding_box\n",
    "        cv2.rectangle(\n",
    "            canvas,\n",
    "            (left, top), (right, bottom),\n",
    "            (0, 255, 0),\n",
    "            6\n",
    "        )\n",
    "        \n",
    "    return canvas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up session:\n",
    "from os.path import join, basename, splitext\n",
    "\n",
    "# Detect:\n",
    "for image_filename in glob.glob(conf.test_dataset)[-1:]:\n",
    "    # Load:\n",
    "    image = cv2.imread(image_filename)\n",
    "    \n",
    "    # Detect:\n",
    "    image_raw = detect_vehicle(image, detector, None)\n",
    "    image_filtered = detect_vehicle(image, detector, 2)#conf.heat_thresh)\n",
    "    \n",
    "    # Save:\n",
    "    name, ext = splitext(basename(image_filename))\n",
    "    for process_type, image_processed in zip((\"raw\", \"filtered\"), (image_raw, image_filtered)):\n",
    "        cv2.imwrite(\n",
    "            join(\n",
    "                conf.output_path, \n",
    "                \"{}-{}{}\".format(\n",
    "                    name,\n",
    "                    process_type,\n",
    "                    ext\n",
    "                )\n",
    "            ),\n",
    "            image_processed\n",
    "        )\n",
    "    \n",
    "    print(\"[{}]: Done\".format(name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detection on static images:\n",
    "for file_id in range(6):\n",
    "    image_detection_demo = plt.figure(figsize=(16, 9))\n",
    "    # Direct detection:\n",
    "    ax=image_detection_demo.add_subplot(1,2,1)\n",
    "    plt.imshow(\n",
    "        mpimg.imread(\n",
    "            \"output_images/test{}-raw.jpg\".format(file_id + 1)\n",
    "        )\n",
    "    )\n",
    "    ax.set_title(\"Test Case {}--Direct Detection\".format(file_id + 1))    \n",
    "    # Filtered detection:\n",
    "    ax=image_detection_demo.add_subplot(1,2,2)\n",
    "    plt.imshow(\n",
    "        mpimg.imread(\n",
    "            \"output_images/test{}-filtered.jpg\".format(file_id + 1)\n",
    "        )\n",
    "    )\n",
    "    ax.set_title(\"Test Case {}--Filtered Detection\".format(file_id + 1))    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test on Videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import everything needed to edit/save/watch video clips\n",
    "from moviepy.editor import VideoFileClip\n",
    "from scipy.ndimage.measurements import label\n",
    "from collections import deque\n",
    "from multiprocessing import Pool\n",
    "from moviepy.editor import concatenate_videoclips\n",
    "from IPython.display import HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spatial temporal filtering demo:\n",
    "def extract_bounding_boxes_and_heat_map(image):\n",
    "    # Initialize heatmap:\n",
    "    H, W, _ = image.shape\n",
    "    heatmap = np.zeros((H, W), dtype=np.int)\n",
    "    \n",
    "    # Detect:\n",
    "    bounding_boxes = detector.detect(\n",
    "        image\n",
    "    )\n",
    "    \n",
    "    # Process:\n",
    "    for bounding_box in bounding_boxes:\n",
    "        # Aggregate heat:\n",
    "        (top, bottom, left, right) = bounding_box\n",
    "        heatmap[top:bottom, left:right] += 1\n",
    "        # Draw:\n",
    "        cv2.rectangle(\n",
    "            image,\n",
    "            (left, top), (right, bottom),\n",
    "            (0, 255, 0),\n",
    "            6\n",
    "        )\n",
    "    \n",
    "    return (image, heatmap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Spatial temporal filtering demo:\n",
    "def extract_labelled_and_external_bounding_box(image):\n",
    "    # Set up session:\n",
    "    from scipy.ndimage.measurements import label\n",
    "    \n",
    "    # Initialize heatmap:\n",
    "    H, W, _ = image.shape\n",
    "    heatmap = np.zeros((H, W), dtype=np.int)\n",
    "    \n",
    "    # Detect:\n",
    "    bounding_boxes = detector.detect(\n",
    "        image\n",
    "    )\n",
    "\n",
    "    # Process:\n",
    "    for bounding_box in bounding_boxes:\n",
    "        # Aggregate heat:\n",
    "        (top, bottom, left, right) = bounding_box\n",
    "        heatmap[top:bottom, left:right] += 1   \n",
    "    \n",
    "    # Filter:\n",
    "    heatmap[heatmap <= conf.heat_thresh] = 0\n",
    "\n",
    "    # Label it:\n",
    "    labelled, num_components = label(heatmap)\n",
    "\n",
    "    # Identify external bounding boxes:\n",
    "    external_bounding_boxes = []\n",
    "    for component_id in range(1, num_components + 1):\n",
    "        # Find pixels with each car_number label value\n",
    "        nonzero = (labelled == component_id).nonzero()\n",
    "        # Identify x and y values of those pixels\n",
    "        nonzero_y, nonzero_x = np.array(nonzero[0]), np.array(nonzero[1])\n",
    "        # Define a bounding box based on min/max x and y\n",
    "        external_bounding_boxes.append(\n",
    "            (\n",
    "                np.min(nonzero_y),\n",
    "                np.max(nonzero_y),\n",
    "                np.min(nonzero_x),\n",
    "                np.max(nonzero_x)\n",
    "            )\n",
    "        )\n",
    "\n",
    "    # Draw:\n",
    "    for bounding_box in external_bounding_boxes:\n",
    "        (top, bottom, left, right) = bounding_box\n",
    "        cv2.rectangle(\n",
    "            image,\n",
    "            (left, top), (right, bottom),\n",
    "            (0, 255, 0),\n",
    "            6\n",
    "        )\n",
    "    \n",
    "    return (image, labelled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for frame_id in range(conf.spatial_filtering_filter_len):\n",
    "    # Load frame:\n",
    "    frame_filename = \"test_video_frames/test-video-frame-{}.jpg\".format(frame_id + 1)\n",
    "    frame = cv2.imread(frame_filename)\n",
    "    \n",
    "    # Get bounding boxes plot and heatmap:\n",
    "    (boxes_image, heatmap) = extract_bounding_boxes_and_heat_map(frame)\n",
    "    \n",
    "    # Initialize canvas:\n",
    "    spatial_temporal_filtering_demo = plt.figure(figsize=(16, 9))\n",
    "    \n",
    "    # Direct detection:\n",
    "    ax=spatial_temporal_filtering_demo.add_subplot(1,2,1)\n",
    "    plt.imshow(\n",
    "        cv2.cvtColor(boxes_image, cv2.COLOR_BGR2RGB)\n",
    "    )\n",
    "    ax.set_title(\"Frame {}--Bounding Boxes\".format(frame_id + 1))\n",
    "    \n",
    "    # Direct detection:\n",
    "    ax=spatial_temporal_filtering_demo.add_subplot(1,2,2)\n",
    "    plt.imshow(\n",
    "        heatmap\n",
    "    )\n",
    "    ax.set_title(\"Frame {}--Heatmap\".format(frame_id + 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load frame:\n",
    "frame_filename = \"test_video_frames/test-video-frame-7.jpg\"\n",
    "frame = cv2.imread(frame_filename)\n",
    "    \n",
    "# Get bounding boxes plot and heatmap:\n",
    "(boxes_image, labelled) = extract_labelled_and_external_bounding_box(frame)\n",
    "    \n",
    "# Initialize canvas:\n",
    "spatial_temporal_filtering_demo = plt.figure(figsize=(16, 9))\n",
    "    \n",
    "# Direct detection:\n",
    "ax=spatial_temporal_filtering_demo.add_subplot(1,2,1)\n",
    "plt.imshow(\n",
    "    cv2.cvtColor(boxes_image, cv2.COLOR_BGR2RGB)\n",
    ")\n",
    "ax.set_title(\"Frame 7--Filtered Bounding Boxes\")\n",
    "    \n",
    "# Direct detection:\n",
    "ax=spatial_temporal_filtering_demo.add_subplot(1,2,2)\n",
    "plt.imshow(\n",
    "    labelled\n",
    ")\n",
    "ax.set_title(\"Frame 7--Labelled\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Static variable decorator:\n",
    "def static_vars(**kwargs):\n",
    "    def decorate(func):\n",
    "        for k in kwargs:\n",
    "            setattr(func, k, kwargs[k])\n",
    "        return func\n",
    "    return decorate\n",
    "\n",
    "# Frame processor:\n",
    "@static_vars(\n",
    "    TEMPORAL_FILTER_LEN=conf.spatial_filtering_filter_len,\n",
    "    bounding_boxes_queue=deque(), \n",
    "    heatmap_accumulator = np.zeros(\n",
    "        tuple(conf.spatial_filtering_frame_size), \n",
    "        dtype=np.int\n",
    "    )\n",
    ")\n",
    "def process_frame(frame):\n",
    "    \"\"\" Detect vehicles in given frame\n",
    "    \"\"\"\n",
    "    # Format:\n",
    "    frame = cv2.cvtColor(frame, cv2.COLOR_RGB2BGR)\n",
    "    \n",
    "    # Detect:\n",
    "    bounding_boxes_current = detector.detect(frame)\n",
    "    \n",
    "    # Spatial filtering:\n",
    "    bounding_boxes_current = heatmap_filtering(\n",
    "        frame, \n",
    "        bounding_boxes_current, \n",
    "        conf.heat_thresh\n",
    "    )\n",
    "\n",
    "    # Temporal filtering:\n",
    "    if len(process_frame.bounding_boxes_queue) == process_frame.TEMPORAL_FILTER_LEN:\n",
    "        # Remove left one:\n",
    "        for bounding_box in process_frame.bounding_boxes_queue.popleft():\n",
    "            (top, bottom, left, right) = bounding_box\n",
    "            process_frame.heatmap_accumulator[top:bottom, left:right] -= 1\n",
    "    \n",
    "    # Append:\n",
    "    process_frame.bounding_boxes_queue.append(bounding_boxes_current)\n",
    "        \n",
    "    # Aggregate heat:\n",
    "    for bounding_box in bounding_boxes_current:\n",
    "        (top, bottom, left, right) = bounding_box\n",
    "        process_frame.heatmap_accumulator[top:bottom, left:right] += 1\n",
    "    \n",
    "    # Filter:\n",
    "    heatmap = process_frame.heatmap_accumulator.copy()\n",
    "    heat_thresh = int(0.8 * len(process_frame.bounding_boxes_queue))\n",
    "    heatmap[heatmap <= heat_thresh] = 0\n",
    "\n",
    "    # Label it:\n",
    "    labelled, num_components = label(heatmap)\n",
    "\n",
    "    # Identify external bounding boxes:\n",
    "    bounding_boxes_filtered = []\n",
    "    for component_id in range(1, num_components + 1):\n",
    "        # Find pixels with each car_number label value\n",
    "        nonzero = (labelled == component_id).nonzero()\n",
    "        # Identify x and y values of those pixels\n",
    "        nonzero_y, nonzero_x = np.array(nonzero[0]), np.array(nonzero[1])\n",
    "        # Define a bounding box based on min/max x and y\n",
    "        bounding_boxes_filtered.append(\n",
    "            (\n",
    "                np.min(nonzero_y),\n",
    "                np.max(nonzero_y),\n",
    "                np.min(nonzero_x),\n",
    "                np.max(nonzero_x)\n",
    "            )\n",
    "        )\n",
    "    \n",
    "    # Draw:\n",
    "    for bounding_box in bounding_boxes_filtered:\n",
    "        (top, bottom, left, right) = bounding_box\n",
    "        cv2.rectangle(\n",
    "            frame,\n",
    "            (left, top), (right, bottom),\n",
    "            (0, 255, 0),\n",
    "            6\n",
    "        )\n",
    "        \n",
    "    return cv2.resize(\n",
    "        cv2.cvtColor(frame, cv2.COLOR_BGR2RGB),\n",
    "        (960, 540)\n",
    "    )    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def video_process_worker(worker_id):\n",
    "    # Specify input & output:\n",
    "    input_filename = video_project_input\n",
    "    output_filename = video_project_output.format(worker_id + 1)\n",
    "    \n",
    "    # Get workload:\n",
    "    start, end = 10*worker_id, 10*(worker_id + 1)\n",
    "    \n",
    "    # Process:\n",
    "    clip_project = VideoFileClip(input_filename).subclip(start, end)\n",
    "    clip_project_detected = clip_project.fl_image(process_frame)\n",
    "    clip_project_detected.write_videofile(output_filename, audio=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Video, Shorter One"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# IO config:\n",
    "video_test_input = \"test_video.mp4\"\n",
    "video_test_output = \"output_videos/test_video_detected.mp4\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Process:\n",
    "clip_test = VideoFileClip(video_test_input)\n",
    "clip_test_detected = clip_test.fl_image(process_frame)\n",
    "%time clip_test_detected.write_videofile(video_test_output, audio=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display:\n",
    "HTML(\n",
    "    \"\"\"\n",
    "    <video width=\"960\" height=\"540\" controls>\n",
    "      <source src=\"{0}\">\n",
    "    </video>\n",
    "    \"\"\".format(video_test_output)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Project Video, Longer One"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# IO config:\n",
    "video_project_input = \"project_video.mp4\"\n",
    "video_project_output = \"output_videos/project_video_detected_{}.mp4\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process--parallel:\n",
    "pool = Pool(5)\n",
    "pool.map(video_process_worker, range(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge all clips:\n",
    "clips = [VideoFileClip(video_project_output.format(id + 1)) for id in range(5)]\n",
    "concat_clip = concatenate_videoclips(clips, method=\"chain\")\n",
    "%time concat_clip.write_videofile(video_project_output.format(0), audio=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Display:\n",
    "HTML(\n",
    "    \"\"\"\n",
    "    <video width=\"960\" height=\"540\" controls>\n",
    "      <source src=\"{0}\">\n",
    "    </video>\n",
    "    \"\"\".format(video_project_output)\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
