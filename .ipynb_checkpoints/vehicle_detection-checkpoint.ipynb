{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Self-Driving Car Engineer Nanodegree\n",
    "\n",
    "\n",
    "## Project: **Vehicle Detection** \n",
    "***\n",
    "\n",
    "In this project, classic object detection framework, i.e., sliding window + image pyramid region proposer and HOG feature + linear SVM detector will be implemented and used for vehicle detection.\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Up Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Configuration file:\n",
    "from vehicle_detection.utils.conf import Conf\n",
    "# IO utilities:\n",
    "import random\n",
    "import glob\n",
    "import matplotlib.image as mpimg\n",
    "from vehicle_detection.utils.dataset import to_hdf5, read_hdf5\n",
    "import pickle\n",
    "# Image processing:\n",
    "import numpy as np\n",
    "import cv2\n",
    "from vehicle_detection.extractors import ReshapeTransformer,ColorHistogramTransformer, HOGTransformer, TemplateTransformer\n",
    "# Visualization:\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "conf = Conf(\"conf/vehicles.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore Dataset\n",
    "***\n",
    "\n",
    "First, let's explore the dataset for vehicle classifier building.\n",
    "\n",
    "After viewing samples from the dataset, we know that **all the cars in images have been clearly segmented**. Thus **HOG features can be extracted directly from input image**.\n",
    "\n",
    "We still need to **set the window size for HOG extractor**. Besides, dataset composition should also be evaluated(e.g., whether the dataset is imbalanced) so as to select the proper algorithm for classifier building.\n",
    "\n",
    "The two stats can be attained from the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vehicle images:\n",
    "vehicle_filenames = glob.glob(conf.vehicle_dataset)\n",
    "print(\n",
    "    \"[  Vehicle Images  ]: Num--{}, Dimensions--{}\".format(\n",
    "        len(vehicle_filenames),\n",
    "        np.array(\n",
    "            [mpimg.imread(vehicle_filename).shape for vehicle_filename in vehicle_filenames]\n",
    "        ).mean(axis = 0)\n",
    "    )\n",
    ")\n",
    "# Non-vehicle images:\n",
    "non_vehicle_filenames = glob.glob(conf.non_vehicle_dataset)\n",
    "print(\n",
    "    \"[Non-Vehicle Images]: Num--{}, Dimensions--{}\".format(\n",
    "        len(non_vehicle_filenames),\n",
    "        np.array(\n",
    "            [mpimg.imread(non_vehicle_filename).shape for non_vehicle_filename in non_vehicle_filenames]\n",
    "        ).mean(axis = 0)\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above output we know that:\n",
    "\n",
    "**1. Window size for HOG extractor should be set as 64-by-64;**\n",
    "\n",
    "**2. There are 8792 positive images and 8968 negative images in training dataset. The dataset is approximately balanced.**\n",
    "\n",
    "---\n",
    "\n",
    "Next let's try to identify the best color space for vehicle & non-vehicle color feature extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Set up session:\n",
    "from vehicle_detection.detectors.image_processing import resize\n",
    "from vehicle_detection.utils.visualization import plot_3d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Utilities for color space exploration:\n",
    "def parse_conversion(color_space):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    if color_space == \"HSV\":\n",
    "        return (cv2.COLOR_BGR2HSV, (\"H\", \"S\", \"V\"))\n",
    "    elif color_space == \"Lab\":\n",
    "        return (cv2.COLOR_BGR2Lab, (\"L*\", \"a*\", \"b*\"))\n",
    "    else:\n",
    "        return (cv2.COLOR_BGR2RGB, (\"R\", \"G\", \"B\"))\n",
    "\n",
    "def plot_pixel_distribution(image_filename, color_space):\n",
    "    # Read:\n",
    "    image_BGR = cv2.imread(image_filename)    \n",
    "    \n",
    "    # Parse conversion:\n",
    "    (conversion, channels) = parse_conversion(color_space)\n",
    "\n",
    "    # Convert subsampled image to desired color space(s):\n",
    "    img_RGB = cv2.cvtColor(image_BGR, cv2.COLOR_BGR2RGB)  # OpenCV uses BGR, matplotlib likes RGB\n",
    "    img_color_space = cv2.cvtColor(image_BGR, conversion)\n",
    "    colors = img_RGB / 255.  # scaled to [0, 1], only for plotting\n",
    "\n",
    "    # Plot and show:\n",
    "    plot_3d(img_color_space, colors, axis_labels=channels)\n",
    "    plt.show()\n",
    "\n",
    "def explore_pixel_distribution(vehicle_filenames, non_vehicle_filenames, color_space):\n",
    "    import random\n",
    "    # Vehicles:\n",
    "    plot_pixel_distribution(random.choice(vehicle_filenames), color_space)\n",
    "    # Non-vehicles:\n",
    "    plot_pixel_distribution(random.choice(non_vehicle_filenames), color_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explore_pixel_distribution(vehicle_filenames, non_vehicle_filenames, \"HSV\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Training Dataset\n",
    "\n",
    "***\n",
    "\n",
    "Now let's build the dataset for vehicle classifier training.\n",
    "\n",
    "I have wrapped skimage's hog descriptor as a sklearn Pipeline interface-complied class HOGTransformer\n",
    "\n",
    "Based on previous experience, I choose the following parameters for HOG descriptor:\n",
    "\n",
    "    1. orientations: 9\n",
    "    2. pixels_per_cell: (4, 4)\n",
    "    3. cells_per_block: (2, 2)\n",
    "    4. transform_sqrt: True, use sqrt normalization\n",
    "    5. block_norm: L1\n",
    "\n",
    "The extracted dataset will be saved to local file system as HDF5 file for easy further access."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is the helper function for HOG feature extraction. Simple augmentation through horizontal flipping is implemented to generate more training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Utilities:\n",
    "def downsample(\n",
    "    image_filenames, \n",
    "    sampling_percentange\n",
    "):\n",
    "    \"\"\" Sample image files\n",
    "    \"\"\"\n",
    "    # Down-sample:\n",
    "    image_filenames = np.random.choice(\n",
    "        image_filenames, \n",
    "        int(sampling_percentange * len(image_filenames))\n",
    "    )\n",
    "    \n",
    "    return image_filenames\n",
    "\n",
    "def load_images(\n",
    "    image_filenames,\n",
    "    image_size,\n",
    "    augmentation=True\n",
    "):\n",
    "    \"\"\" Load images\n",
    "    \"\"\"\n",
    "    features = []\n",
    "    \n",
    "    # Extract features:\n",
    "    for image_filename in image_filenames:\n",
    "        # Load and convert to grayscale:\n",
    "        object_image = cv2.resize(\n",
    "            cv2.imread(image_filename),\n",
    "            image_size,\n",
    "            interpolation = cv2.INTER_AREA\n",
    "        )\n",
    "        # Prepare ROIs:\n",
    "        ROIs = (object_image, cv2.flip(object_image, 1)) if augmentation else (object_image,)\n",
    "        # Extract features:\n",
    "        for ROI in ROIs:\n",
    "            features.append(ROI)\n",
    "    \n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Should dataset be extracted:\n",
    "if conf.generate_dataset:\n",
    "    # Load images:\n",
    "    vehicle_images = load_images(\n",
    "        downsample(vehicle_filenames, sampling_percentange=conf.sampling_percentange),\n",
    "        tuple(conf.hog_window_size),\n",
    "        conf.augmentation\n",
    "    )\n",
    "    non_vehicle_images = load_images(\n",
    "        downsample(non_vehicle_filenames, sampling_percentange=conf.sampling_percentange),\n",
    "        tuple(conf.hog_window_size),\n",
    "        conf.augmentation\n",
    "    )\n",
    "    # Training set:\n",
    "    X_train = np.array(vehicle_images + non_vehicle_images)\n",
    "    y_train = np.array([1] * len(vehicle_images) + [-1] * len(non_vehicle_images))\n",
    "    indices = np.arange(len(X_train))\n",
    "    np.random.shuffle(indices)\n",
    "    X_train, y_train = X_train[indices], y_train[indices]\n",
    "    # Shape:\n",
    "    X_train = X_train.reshape(tuple(conf.shape_serialized))\n",
    "    # Dataset info:\n",
    "    print(X_train.shape)\n",
    "    print(y_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Classifier\n",
    "\n",
    "***\n",
    "\n",
    "Here I choose to implement logistic regression & linear SVM using SGDClassifier because the dimensions of training dataset,(35520, 8100), is formidable. Use SVC will lead to a very slow training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross validation:\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "# Classifier:\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "# Evaluation metric:\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import make_scorer\n",
    "# Hyperparameter tuning:\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Model 1--Linear SVC:\n",
    "def get_linear_svc():\n",
    "    # Model:\n",
    "    model = Pipeline(\n",
    "        [\n",
    "            # Deserializer:\n",
    "            ('des', ReshapeTransformer(conf.shape_deserialized)),\n",
    "            # Feature extractor:\n",
    "            ('vec', FeatureUnion(\n",
    "                [\n",
    "                    (\"hog\", HOGTransformer(\n",
    "                        color_space = conf.hog_color_space,\n",
    "                        shape_only = conf.hog_shape_only,\n",
    "                        orientations = conf.hog_orientations,\n",
    "                        pixels_per_cell = tuple(conf.hog_pixels_per_cell),\n",
    "                        cells_per_block = tuple(conf.hog_cells_per_block),\n",
    "                        transform_sqrt = conf.hog_normalize,\n",
    "                        block_norm = str(conf.hog_block_norm)\n",
    "                    )),\n",
    "                ]\n",
    "            )),\n",
    "            # Preprocessor:\n",
    "            ('scl', StandardScaler()),\n",
    "            # Classifier:\n",
    "            ('clf', LinearSVC(\n",
    "                penalty='l2', \n",
    "                loss=conf.classifier_loss,\n",
    "                C=conf.classifier_C,\n",
    "                max_iter=2000\n",
    "            ))\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # Hyperparameters:\n",
    "    params = {\n",
    "        # VEC--hog:\n",
    "        #\"vec__hog__pixels_per_cell\": ((8,8), (16, 16)),\n",
    "        # CLF--learning rate:\n",
    "        #\"clf__loss\": (\"hinge\", \"squared_hinge\"),\n",
    "        # CLF--regularization:\n",
    "        #\"clf__penalty\": (\"l1\", \"l2\")\n",
    "        \"clf__C\": (5e-4, 1e-3)\n",
    "    }\n",
    "    \n",
    "    return (model, params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Model 2--XGBoost:\n",
    "def get_xgboost():\n",
    "    # Model:\n",
    "    model = Pipeline(\n",
    "        [\n",
    "            # Deserializer:\n",
    "            ('des', ReshapeTransformer(conf.shape_deserialized)),\n",
    "            # Feature extractor:\n",
    "            ('vec', FeatureUnion(\n",
    "                [\n",
    "                    # 2. Shape--HOG:\n",
    "                    (\"hog\", HOGTransformer(\n",
    "                        color_space = conf.hog_color_space,\n",
    "                        shape_only = conf.hog_shape_only,\n",
    "                        orientations = conf.hog_orientations,\n",
    "                        pixels_per_cell = tuple(conf.hog_pixels_per_cell),\n",
    "                        cells_per_block = tuple(conf.hog_cells_per_block),\n",
    "                        transform_sqrt = conf.hog_normalize,\n",
    "                        block_norm = str(conf.hog_block_norm)\n",
    "                    )),\n",
    "                ]\n",
    "            )),\n",
    "            # Preprocessor:\n",
    "            ('scl', StandardScaler()),\n",
    "            # Classifier:\n",
    "            ('clf', XGBClassifier(\n",
    "                max_depth=8, \n",
    "                learning_rate=0.1, \n",
    "                n_estimators=1024,\n",
    "                nthread=4\n",
    "            ))\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # Hyperparameters:\n",
    "    params = {\n",
    "        # VEC--hog:\n",
    "        #\"vec__hog__pixels_per_cell\": ((8,8), (16, 16)),\n",
    "        # CLF--learning rate:\n",
    "        #\"clf__learning_rate\": (0.1, 0.3),\n",
    "    }\n",
    "    \n",
    "    return (model, params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Model 3--Logistic regression:\n",
    "def get_logistic():\n",
    "    # Model:\n",
    "    model = Pipeline(\n",
    "        [\n",
    "            # Deserializer:\n",
    "            ('des', ReshapeTransformer(conf.shape_deserialized)),\n",
    "            # Feature extractor:\n",
    "            ('vec', FeatureUnion(\n",
    "                [\n",
    "                    # 2. Shape--HOG:\n",
    "                    (\"hog\", HOGTransformer(\n",
    "                        color_space = conf.hog_color_space,\n",
    "                        shape_only = conf.hog_shape_only,\n",
    "                        orientations = conf.hog_orientations,\n",
    "                        # Optimal--(8, 8):\n",
    "                        pixels_per_cell = tuple(conf.hog_pixels_per_cell),\n",
    "                        cells_per_block = tuple(conf.hog_cells_per_block),\n",
    "                        # Optimal--True:\n",
    "                        transform_sqrt = conf.hog_normalize,\n",
    "                        block_norm = str(conf.hog_block_norm)\n",
    "                    )),\n",
    "                ]\n",
    "            )),\n",
    "            # Preprocessor:\n",
    "            ('scl', StandardScaler()),\n",
    "            # Classifier:\n",
    "            ('clf', LogisticRegression(\n",
    "                penalty='l2', \n",
    "                C=1.0,\n",
    "                n_jobs=4 \n",
    "            ))\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # Hyperparameters:\n",
    "    params = {\n",
    "        # VEC--hog:\n",
    "        #\"vec__hog__pixels_per_cell\": ((8,8), (16, 16)),\n",
    "        # CLF--learning rate:\n",
    "        #\"clf__loss\": (\"hinge\", \"squared_hinge\"),\n",
    "        # CLF--regularization:\n",
    "        #\"clf__penalty\": (\"l1\", \"l2\")\n",
    "        \"clf__C\": (1e-3, 1e-1)\n",
    "    }\n",
    "    \n",
    "    return (model, params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create cross-validation sets from the training data\n",
    "cv_sets_training = StratifiedShuffleSplit(\n",
    "    n_splits = 3, \n",
    "    test_size = 0.20, \n",
    "    random_state = 42\n",
    ").split(X_train, y_train)\n",
    "\n",
    "# Model 1: Linear SVC\n",
    "(model, params) = get_linear_svc()\n",
    "# Model 2: XGBoost:\n",
    "#(model, params) = get_xgboost()\n",
    "# Model 3: Logistic\n",
    "#(model, params) = get_logistic()\n",
    "\n",
    "# Make an scorer object\n",
    "scorer = make_scorer(accuracy_score)\n",
    "\n",
    "# Perform grid search on the classifier using 'scorer' as the scoring method\n",
    "grid_searcher = GridSearchCV(\n",
    "    estimator = model,\n",
    "    param_grid = params,\n",
    "    scoring = scorer,\n",
    "    cv = cv_sets_training,\n",
    "    n_jobs = 2,\n",
    "    verbose = 10\n",
    ")\n",
    "\n",
    "# Fit the grid search object to the training data and find the optimal parameters\n",
    "grid_fitted = grid_searcher.fit(X_train, y_train)\n",
    "\n",
    "# Get parameters & scores:\n",
    "best_parameters, score, _ = max(grid_fitted.grid_scores_, key=lambda x: x[1])\n",
    "\n",
    "# Display result:\n",
    "print(\n",
    "    \"[Best Parameters]: {}\\n[Best Score]: {}\".format(\n",
    "        best_parameters, score\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"[Train & Calibrate Best Model]: ...\")\n",
    "# Get the best model\n",
    "best_model = grid_fitted.best_estimator_\n",
    "best_model.set_params(**best_parameters)\n",
    "\n",
    "# Train on whole dataset with best parameters and probability calibration:\n",
    "best_model_calibrated = CalibratedClassifierCV(best_model, cv=3)\n",
    "best_model_calibrated.fit(X_train, y_train)\n",
    "print(\"[Train & Calibrate Best Model]: Done.\")\n",
    "\n",
    "# Save model:\n",
    "with open(conf.classifier_path, 'wb') as model_pkl:\n",
    "    pickle.dump(best_model_calibrated, model_pkl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vehicle Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Set up session:\n",
    "from vehicle_detection.detectors import SlidingWindowPyramidDetector\n",
    "from vehicle_detection.detectors import non_maxima_suppression\n",
    "from vehicle_detection.detectors import heatmap_filtering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Detector:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Initialize detector:\n",
    "detector = SlidingWindowPyramidDetector(\n",
    "    conf\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test on Static Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Utilities:\n",
    "def detect_vehicle(image, detector, heat_thresh=None):    \n",
    "    # Detect:\n",
    "    bounding_boxes = detector.detect(\n",
    "        image\n",
    "    )\n",
    "    \n",
    "    # Heatmap filtering:\n",
    "    if not heat_thresh is None:\n",
    "        bounding_boxes = heatmap_filtering(image, bounding_boxes, heat_thresh)\n",
    "        \n",
    "    # Draw:\n",
    "    canvas = image.copy()\n",
    "    for bounding_box in bounding_boxes:\n",
    "        (top, bottom, left, right) = bounding_box\n",
    "        cv2.rectangle(\n",
    "            canvas,\n",
    "            (left, top), (right, bottom),\n",
    "            (0, 255, 0),\n",
    "            6\n",
    "        )\n",
    "        \n",
    "    return canvas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up session:\n",
    "from os.path import join, basename, splitext\n",
    "\n",
    "for image_filename in glob.glob(conf.test_dataset)[-1:]:\n",
    "    # Load:\n",
    "    image = cv2.imread(image_filename)\n",
    "    \n",
    "    # Detect:\n",
    "    image_raw = detect_vehicle(image, detector, None)\n",
    "    image_filtered = detect_vehicle(image, detector, 2)#conf.heat_thresh)\n",
    "    \n",
    "    # Save:\n",
    "    name, ext = splitext(basename(image_filename))\n",
    "    for process_type, image_processed in zip((\"raw\", \"filtered\"), (image_raw, image_filtered)):\n",
    "        cv2.imwrite(\n",
    "            join(\n",
    "                conf.output_path, \n",
    "                \"{}-{}{}\".format(\n",
    "                    name,\n",
    "                    process_type,\n",
    "                    ext\n",
    "                )\n",
    "            ),\n",
    "            image_processed\n",
    "        )\n",
    "    \n",
    "    print(\"[{}]: Done\".format(name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test on Videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import everything needed to edit/save/watch video clips\n",
    "from moviepy.editor import VideoFileClip\n",
    "from scipy.ndimage.measurements import label\n",
    "from collections import deque\n",
    "from multiprocessing import Pool\n",
    "from moviepy.editor import concatenate_videoclips\n",
    "from IPython.display import HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Static variable decorator:\n",
    "def static_vars(**kwargs):\n",
    "    def decorate(func):\n",
    "        for k in kwargs:\n",
    "            setattr(func, k, kwargs[k])\n",
    "        return func\n",
    "    return decorate\n",
    "\n",
    "# Frame processor:\n",
    "@static_vars(\n",
    "    TEMPORAL_FILTER_LEN=conf.spatial_filtering_filter_len,\n",
    "    bounding_boxes_queue=deque(), \n",
    "    heatmap_accumulator = np.zeros(\n",
    "        tuple(conf.spatial_filtering_frame_size), \n",
    "        dtype=np.int\n",
    "    )\n",
    ")\n",
    "def process_frame(frame):\n",
    "    \"\"\" Detect vehicles in given frame\n",
    "    \"\"\"\n",
    "    # Format:\n",
    "    frame = cv2.cvtColor(frame, cv2.COLOR_RGB2BGR)\n",
    "    \n",
    "    # Detect:\n",
    "    bounding_boxes_current = detector.detect(frame)\n",
    "    \n",
    "    # Spatial filtering:\n",
    "    bounding_boxes_current = heatmap_filtering(\n",
    "        frame, \n",
    "        bounding_boxes_current, \n",
    "        conf.heat_thresh\n",
    "    )\n",
    "\n",
    "    # Temporal filtering:\n",
    "    if len(process_frame.bounding_boxes_queue) == process_frame.TEMPORAL_FILTER_LEN:\n",
    "        # Remove left one:\n",
    "        for bounding_box in process_frame.bounding_boxes_queue.popleft():\n",
    "            (top, bottom, left, right) = bounding_box\n",
    "            process_frame.heatmap_accumulator[top:bottom, left:right] -= 1\n",
    "    \n",
    "    # Append:\n",
    "    process_frame.bounding_boxes_queue.append(bounding_boxes_current)\n",
    "        \n",
    "    # Aggregate heat:\n",
    "    for bounding_box in bounding_boxes_current:\n",
    "        (top, bottom, left, right) = bounding_box\n",
    "        process_frame.heatmap_accumulator[top:bottom, left:right] += 1\n",
    "    \n",
    "    # Filter:\n",
    "    heatmap = process_frame.heatmap_accumulator.copy()\n",
    "    heat_thresh = int(0.8 * len(process_frame.bounding_boxes_queue))\n",
    "    heatmap[heatmap <= heat_thresh] = 0\n",
    "\n",
    "    # Label it:\n",
    "    labelled, num_components = label(heatmap)\n",
    "\n",
    "    # Identify external bounding boxes:\n",
    "    bounding_boxes_filtered = []\n",
    "    for component_id in range(1, num_components + 1):\n",
    "        # Find pixels with each car_number label value\n",
    "        nonzero = (labelled == component_id).nonzero()\n",
    "        # Identify x and y values of those pixels\n",
    "        nonzero_y, nonzero_x = np.array(nonzero[0]), np.array(nonzero[1])\n",
    "        # Define a bounding box based on min/max x and y\n",
    "        bounding_boxes_filtered.append(\n",
    "            (\n",
    "                np.min(nonzero_y),\n",
    "                np.max(nonzero_y),\n",
    "                np.min(nonzero_x),\n",
    "                np.max(nonzero_x)\n",
    "            )\n",
    "        )\n",
    "    \n",
    "    # Draw:\n",
    "    for bounding_box in bounding_boxes_filtered:\n",
    "        (top, bottom, left, right) = bounding_box\n",
    "        cv2.rectangle(\n",
    "            frame,\n",
    "            (left, top), (right, bottom),\n",
    "            (0, 255, 0),\n",
    "            6\n",
    "        )\n",
    "        \n",
    "    return cv2.resize(\n",
    "        cv2.cvtColor(frame, cv2.COLOR_BGR2RGB),\n",
    "        (960, 540)\n",
    "    )    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def video_process_worker(worker_id):\n",
    "    # Specify input & output:\n",
    "    input_filename = video_project_input\n",
    "    output_filename = video_project_output.format(worker_id + 1)\n",
    "    \n",
    "    # Get workload:\n",
    "    start, end = 10*worker_id, 10*(worker_id + 1)\n",
    "    \n",
    "    # Process:\n",
    "    clip_project = VideoFileClip(input_filename).subclip(start, end)\n",
    "    clip_project_detected = clip_project.fl_image(process_frame)\n",
    "    clip_project_detected.write_videofile(output_filename, audio=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Video, Shorter One"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# IO config:\n",
    "video_test_input = \"test_video.mp4\"\n",
    "video_test_output = \"output_videos/test_video_detected.mp4\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Process:\n",
    "clip_test = VideoFileClip(video_test_input)\n",
    "clip_test_detected = clip_test.fl_image(process_frame)\n",
    "%time clip_test_detected.write_videofile(video_test_output, audio=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display:\n",
    "HTML(\n",
    "    \"\"\"\n",
    "    <video width=\"960\" height=\"540\" controls>\n",
    "      <source src=\"{0}\">\n",
    "    </video>\n",
    "    \"\"\".format(video_test_output)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Project Video, Longer One"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# IO config:\n",
    "video_project_input = \"project_video.mp4\"\n",
    "video_project_output = \"output_videos/project_video_detected_{}.mp4\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[MoviePy] >>>> Building video output_videos/project_video_detected_3.mp4\n",
      "[MoviePy] Writing video output_videos/project_video_detected_3.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/251 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[MoviePy] >>>> Building video output_videos/project_video_detected_4.mp4\n",
      "[MoviePy] Writing video output_videos/project_video_detected_4.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/251 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[MoviePy] >>>> Building video output_videos/project_video_detected_2.mp4\n",
      "[MoviePy] Writing video output_videos/project_video_detected_2.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/251 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[MoviePy] >>>> Building video output_videos/project_video_detected_1.mp4\n",
      "[MoviePy] Writing video output_videos/project_video_detected_1.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/251 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[MoviePy] >>>> Building video output_videos/project_video_detected_5.mp4\n",
      "[MoviePy] Writing video output_videos/project_video_detected_5.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████▉| 250/251 [3:11:04<00:47, 47.26s/it]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[MoviePy] Done.\n",
      "[MoviePy] >>>> Video ready: output_videos/project_video_detected_1.mp4 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████▉| 250/251 [3:11:56<00:45, 45.84s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[MoviePy] Done.\n",
      "[MoviePy] >>>> Video ready: output_videos/project_video_detected_3.mp4 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████▉| 250/251 [3:12:09<00:45, 45.12s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[MoviePy] Done.\n",
      "[MoviePy] >>>> Video ready: output_videos/project_video_detected_5.mp4 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████▉| 250/251 [3:12:54<00:42, 42.49s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[MoviePy] Done.\n",
      "[MoviePy] >>>> Video ready: output_videos/project_video_detected_2.mp4 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████▉| 250/251 [3:13:05<00:41, 41.96s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[MoviePy] Done.\n",
      "[MoviePy] >>>> Video ready: output_videos/project_video_detected_4.mp4 \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[None, None, None, None, None]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Process--parallel:\n",
    "pool = Pool(5)\n",
    "pool.map(video_process_worker, range(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[MoviePy] >>>> Building video output_videos/project_video_detected_0.mp4\n",
      "[MoviePy] Writing video output_videos/project_video_detected_0.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████▉| 1250/1251 [00:12<00:00, 97.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[MoviePy] Done.\n",
      "[MoviePy] >>>> Video ready: output_videos/project_video_detected_0.mp4 \n",
      "\n",
      "CPU times: user 1.11 s, sys: 760 ms, total: 1.87 s\n",
      "Wall time: 13.2 s\n"
     ]
    }
   ],
   "source": [
    "# Merge all clips:\n",
    "clips = [VideoFileClip(video_project_output.format(id + 1)) for id in range(5)]\n",
    "concat_clip = concatenate_videoclips(clips, method=\"chain\")\n",
    "%time concat_clip.write_videofile(video_project_output.format(0), audio=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Display:\n",
    "HTML(\n",
    "    \"\"\"\n",
    "    <video width=\"960\" height=\"540\" controls>\n",
    "      <source src=\"{0}\">\n",
    "    </video>\n",
    "    \"\"\".format(video_project_output)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Improve the draw_lines() function\n",
    "\n",
    "**At this point, if you were successful with making the pipeline and tuning parameters, you probably have the Hough line segments drawn onto the road, but what about identifying the full extent of the lane and marking it clearly as in the example video (P1_example.mp4)?  Think about defining a line to run the full length of the visible lane based on the line segments you identified with the Hough Transform. As mentioned previously, try to average and/or extrapolate the line segments you've detected to map out the full extent of the lane lines. You can see an example of the result you're going for in the video \"P1_example.mp4\".**\n",
    "\n",
    "**Go back and modify your draw_lines function accordingly and try re-running your pipeline. The new output should draw a single, solid line over the left lane line and a single, solid line over the right lane line. The lines should start from the bottom of the image and extend out to the top of the region of interest.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now for the one with the solid yellow lane on the left. This one's more tricky!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "yellow_output = 'test_videos_output/solidYellowLeft.mp4'\n",
    "## To speed up the testing process you may want to try your pipeline on a shorter subclip of the video\n",
    "## To do so add .subclip(start_second,end_second) to the end of the line below\n",
    "## Where start_second and end_second are integer values representing the start and end of the subclip\n",
    "## You may also uncomment the following line for a subclip of the first 5 seconds\n",
    "##clip2 = VideoFileClip('test_videos/solidYellowLeft.mp4').subclip(0,5)\n",
    "clip2 = VideoFileClip('test_videos/solidYellowLeft.mp4')\n",
    "yellow_clip = clip2.fl_image(process_image)\n",
    "%time yellow_clip.write_videofile(yellow_output, audio=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "HTML(\"\"\"\n",
    "<video width=\"960\" height=\"540\" controls>\n",
    "  <source src=\"{0}\">\n",
    "</video>\n",
    "\"\"\".format(yellow_output))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Writeup and Submission\n",
    "\n",
    "If you're satisfied with your video outputs, it's time to make the report writeup in a pdf or markdown file. Once you have this Ipython notebook ready along with the writeup, it's time to submit for review! Here is a [link](https://github.com/udacity/CarND-LaneLines-P1/blob/master/writeup_template.md) to the writeup template file.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Optional Challenge\n",
    "\n",
    "Try your lane finding pipeline on the video below.  Does it still work?  Can you figure out a way to make it more robust?  If you're up for the challenge, modify your pipeline so it works with this video and submit it along with the rest of your project!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "challenge_output = 'test_videos_output/challenge.mp4'\n",
    "## To speed up the testing process you may want to try your pipeline on a shorter subclip of the video\n",
    "## To do so add .subclip(start_second,end_second) to the end of the line below\n",
    "## Where start_second and end_second are integer values representing the start and end of the subclip\n",
    "## You may also uncomment the following line for a subclip of the first 5 seconds\n",
    "clip3 = VideoFileClip('test_videos/challenge.mp4').subclip(0,5)\n",
    "# clip3 = VideoFileClip('test_videos/challenge.mp4')\n",
    "challenge_clip = clip3.fl_image(process_image)\n",
    "%time challenge_clip.write_videofile(challenge_output, audio=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "HTML(\"\"\"\n",
    "<video width=\"960\" height=\"540\" controls>\n",
    "  <source src=\"{0}\">\n",
    "</video>\n",
    "\"\"\".format(challenge_output))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
